name: Spark History

on:
  workflow_dispatch:

permissions:
  contents: read

env:
  CLUSTER_NAME: spark-history
  SPARK_HISTORY_BUCKET: spark-history-bucket
  DATAPROC_STG_BUCKET: gs://dataproc-stg-bucket
  DATAPROC_TEMP_BUCKET: gs://dataproc-tmp-bucket
  DATAPROC_IMAGE: 2.2.2-debian12

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - uses: 'google-github-actions/auth@v2'
      with:
        credentials_json: '${{ secrets.GCP_DEMOSSAK }}'

    - name: 'Set up Cloud SDK'
      uses: 'google-github-actions/setup-gcloud@v2'

    - name: Create spark history
      run: |
        gcloud dataproc clusters create ${{ env.CLUSTER_NAME }} --region=us --image-version=${{ env.DATAPROC_IMAGE }} --master-machine-type=n2-standard-2 --num-workers=0 --bucket=${{ env.DATAPROC_STG_BUCKET }} --temp-bucket=${{ env.DATAPROC_TEMP_BUCKET }} --enable-component-gateway --properties=dataproc:dataproc.allow.zero.workers=true,yarn:yarn.nodemanager.remote-app-log-dir=gs://${{ env.SPARK_HISTORY_BUCKET }}/*/yarn-logs,spark:spark.history.fs.logDirectory=gs://${{ env.SPARK_HISTORY_BUCKET }}/*/spark-job-history,spark:spark.history.custom.executor.log.url.applyIncompleteApplication=false,spark:spark.history.custom.executor.log.url="{{ '{{YARN_LOG_SERVER_URL}}/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}' }}"
