name: Run Dataproc with a workflow-template file

on:
  workflow_call:
    inputs:
      region:
        description: 'Region'
        required: true
        type: string
      workflowTemplateFilePath:
        description: 'Workflow file path'
        required: true
        type: string
      workflowTemplateId:
        description: 'Workflow template id'
        required: true
        type: string
      sparkHistoryBucket:
        description: 'Spark History bucket path'
        required: true
        type: string

permissions:
  contents: read

jobs:
  deploy_on_dataproc:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_DEMOSSAK }}'

      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'

      - name: Init spark history logs folders in GCS
        run: |
          sh ./src/main/resources/bin/init_GCS_folder.sh ${{ inputs.sparkHistoryBucket }}/${{ inputs.workflowTemplateId }}/spark-job-history
          sh ./src/main/resources/bin/init_GCS_folder.sh ${{ inputs.sparkHistoryBucket }}/${{ inputs.workflowTemplateId }}/yarn-logs

      - name: Customize workflow-template (clusterName = spark history logs folder = workflow template ID)
        env:
          SPARK_HISTORY_PATH: ${{ inputs.sparkHistoryBucket }}/${{ inputs.workflowTemplateId }}
        run: | 
          sed -i 's/@@CLUSTER_NAME/${{ inputs.workflowTemplateId }}/g; s|@@SPARK_HISTORY_PATH|${{ env.SPARK_HISTORY_PATH }}|g' ${{ inputs.workflowTemplateFilePath }}

      - name: Run dataproc import
        run: |
          gcloud dataproc workflow-templates import ${{ inputs.workflowTemplateId }} --region=${{ inputs.region }} --source=${{ inputs.workflowTemplateFilePath }} --quiet

      - name: Run dataproc instantiate
        run: |
          gcloud dataproc workflow-templates instantiate ${{ inputs.workflowTemplateId }} --region=${{ inputs.region }}