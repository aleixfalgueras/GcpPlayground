jobs:
  - sparkJob:
      jarFileUris:
        - gs://demos-bucket-gcpplay/jars/GcpPlayground-assembly-0.1.0-SNAPSHOT.jar
      mainClass: com.demos.dataproc.etl.SparkEtlApp
      args:
        - "--env=dev"
        - "--executionMode=GCP"
        - "--etl=@@ETL"
        - "--targetRepo=@@TARGET_REPO"
      properties: # [2e, 2C, 2G, 6C (4 + 2), 6G (4 + 2)]
        spark.driver.cores: "2"
        spark.driver.memory: "2G"
        spark.executor.instances: "2"
        spark.executor.memory: "2G"
        spark.executor.cores: "2"
        spark.dynamicAllocation.enabled: "false"
        # jvm logs timezone config
        spark.driver.extraJavaOptions: "-Duser.timezone=Europe/Sofia"
        spark.executor.extraJavaOptions: "-Duser.timezone=Europe/Sofia"
        # properties to enable and save spark history logs
        spark.eventLog.enabled: "true"
        spark.eventLog.dir: "gs://@@SPARK_HISTORY_PATH/spark-job-history"
        spark.hadoop.yarn.nodemanager.remote-app-log-dir: "gs://@@SPARK_HISTORY_PATH/yarn-logs"
        spark.history.fs.logDirectory: "gs://@@SPARK_HISTORY_PATH/spark-job-history"
        spark.history.fs.gs.outputstream.type: "FLUSHABLE_COMPOSITE"
        spark.history.fs.gs.outputstream.sync.min.interval.ms: "5000ms"
    stepId: compute
placement:
  managedCluster: # [2w, 2c, 6G, 4c, 12G]
    clusterName: @@CLUSTER_NAME
    config:
      gceClusterConfig:
        internalIpOnly: false
      initializationActions:
        "executableFile": "gs://demos-bucket-gcpplay/init_actions/set_timezone.sh"
        "executionTimeout": "300s"
      endpointConfig:
        enableHttpPortAccess: true
      configBucket: dataproc-stg-bucket-gcpplay
      tempBucket: dataproc-tmp-bucket-gcpplay
      softwareConfig:
        imageVersion: 2.2-debian12
      masterConfig:
        numInstances: 1
        machineTypeUri: 'e2-standard-2'
        diskConfig:
          bootDiskSizeGb: 100
      workerConfig:
        numInstances: 2
        machineTypeUri: 'e2-standard-2'
        diskConfig:
          bootDiskSizeGb: 100